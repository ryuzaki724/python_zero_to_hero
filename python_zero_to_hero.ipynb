{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad5a264a",
   "metadata": {},
   "source": [
    "# Python Zero To Hero\n",
    "\n",
    "## A comprehensive tutorial covering:\n",
    "1. **Python Basics** - Variables, data types, I/O\n",
    "2. **Data Structures** - Lists, tuples, dictionaries\n",
    "3. **Functions and Classes** - Building blocks of code\n",
    "4. **Neural Network from Scratch** - Understanding the fundamentals\n",
    "5. **PyTorch Regression** - Predicting continuous values\n",
    "6. **PyTorch Classification** - Predicting categories\n",
    "7. **GPT from Scratch** - Building a language model\n",
    "\n",
    "Written for beginners, with full explanations and docstrings!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be7ece1",
   "metadata": {},
   "source": [
    "## Part 0: Install Required Libraries\n",
    "\n",
    "Run this cell first to install all necessary packages!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "05c28b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in ./venv/lib/python3.14/site-packages (2.4.1)\n",
      "Requirement already satisfied: pandas in ./venv/lib/python3.14/site-packages (2.3.3)\n",
      "Requirement already satisfied: matplotlib in ./venv/lib/python3.14/site-packages (3.10.8)\n",
      "Requirement already satisfied: torch in ./venv/lib/python3.14/site-packages (2.9.1)\n",
      "Requirement already satisfied: scikit-learn in ./venv/lib/python3.14/site-packages (1.8.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.14/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.14/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./venv/lib/python3.14/site-packages (from pandas) (2025.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./venv/lib/python3.14/site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in ./venv/lib/python3.14/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./venv/lib/python3.14/site-packages (from matplotlib) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./venv/lib/python3.14/site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.14/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in ./venv/lib/python3.14/site-packages (from matplotlib) (12.1.0)\n",
      "Requirement already satisfied: pyparsing>=3 in ./venv/lib/python3.14/site-packages (from matplotlib) (3.3.1)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.14/site-packages (from torch) (3.20.3)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./venv/lib/python3.14/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in ./venv/lib/python3.14/site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./venv/lib/python3.14/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in ./venv/lib/python3.14/site-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.14/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in ./venv/lib/python3.14/site-packages (from torch) (2026.1.0)\n",
      "Requirement already satisfied: scipy>=1.10.0 in ./venv/lib/python3.14/site-packages (from scikit-learn) (1.17.0)\n",
      "Requirement already satisfied: joblib>=1.3.0 in ./venv/lib/python3.14/site-packages (from scikit-learn) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in ./venv/lib/python3.14/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.14/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./venv/lib/python3.14/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.14/site-packages (from jinja2->torch) (3.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages (run this cell first!)\n",
    "%pip install numpy pandas matplotlib torch scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a77aa52",
   "metadata": {},
   "source": [
    "# Part 1: Python Basics ðŸŽ¯\n",
    "\n",
    "## 1.1 Input and Output (I/O)\n",
    "\n",
    "**Output** = Computer talking TO you (`print()`)\n",
    "**Input** = Computer listening FROM you (`input()`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ee352bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, World!\n",
      "Welcome to Python!\n",
      "My favorite number is 7\n",
      "My name is Alex and I am 14 years old.\n"
     ]
    }
   ],
   "source": [
    "# OUTPUT - Using print()\n",
    "print(\"Hello, World!\")\n",
    "print(\"Welcome to Python!\")\n",
    "\n",
    "# Print multiple things\n",
    "print(\"My favorite number is\", 7)\n",
    "\n",
    "# F-strings (formatted strings)\n",
    "name = \"Alex\"\n",
    "age = 14\n",
    "print(f\"My name is {name} and I am {age} years old.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "70ce2c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice to meet you, Cora!\n",
      "In 10 years, you'll be 27 years old!\n"
     ]
    }
   ],
   "source": [
    "# INPUT - Getting info from user\n",
    "# IMPORTANT: Enter a NUMBER when prompted!\n",
    "user_name = input(\"What is your name? \")\n",
    "print(f\"Nice to meet you, {user_name}!\")\n",
    "\n",
    "age_string = input(\"How old are you? (enter a number) \")\n",
    "age_number = int(age_string)\n",
    "print(f\"In 10 years, you'll be {age_number + 10} years old!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011fa16a",
   "metadata": {},
   "source": [
    "## 1.2 Variables and Constants\n",
    "\n",
    "**Variables** = Labeled boxes that CAN change\n",
    "**Constants** = Values that should NOT change (ALL CAPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "087cb11b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting score: 0\n",
      "After bonus: 10\n",
      "After another bonus: 15\n",
      "\n",
      "Player: SuperCoder, Health: 100, Speed: 5.5, Alive: True\n"
     ]
    }
   ],
   "source": [
    "# VARIABLES - Can change!\n",
    "score = 0\n",
    "print(f\"Starting score: {score}\")\n",
    "\n",
    "score = score + 10\n",
    "print(f\"After bonus: {score}\")\n",
    "\n",
    "score += 5  # Shorthand\n",
    "print(f\"After another bonus: {score}\")\n",
    "\n",
    "# Different data types\n",
    "player_name = \"SuperCoder\"  # String\n",
    "health = 100                # Integer\n",
    "speed = 5.5                 # Float\n",
    "is_alive = True             # Boolean\n",
    "\n",
    "print(f\"\\nPlayer: {player_name}, Health: {health}, Speed: {speed}, Alive: {is_alive}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "9ffcef24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PI = 3.14159\n",
      "Gravity = 9.8 m/sÂ²\n",
      "Max Players = 10\n",
      "\n",
      "Circle with radius 5 has area 78.54\n"
     ]
    }
   ],
   "source": [
    "# CONSTANTS - Should NOT change (ALL CAPS)\n",
    "PI = 3.14159\n",
    "GRAVITY = 9.8\n",
    "MAX_PLAYERS = 10\n",
    "GAME_TITLE = \"Python Adventure\"\n",
    "\n",
    "print(f\"PI = {PI}\")\n",
    "print(f\"Gravity = {GRAVITY} m/sÂ²\")\n",
    "print(f\"Max Players = {MAX_PLAYERS}\")\n",
    "\n",
    "# Use constant in calculation\n",
    "radius = 5\n",
    "area = PI * radius * radius\n",
    "print(f\"\\nCircle with radius {radius} has area {area:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da84797f",
   "metadata": {},
   "source": [
    "## 1.3 Data Types\n",
    "\n",
    "| Type | Description | Example |\n",
    "|------|-------------|---------|\n",
    "| `int` | Whole numbers | `42`, `-7` |\n",
    "| `float` | Decimals | `3.14`, `-0.5` |\n",
    "| `str` | Text | `\"Hello\"` |\n",
    "| `bool` | True/False | `True`, `False` |\n",
    "| `None` | No value | `None` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "4fc2f60d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42 is type: <class 'int'>\n",
      "3.14 is type: <class 'float'>\n",
      "Hello Python! is type: <class 'str'>\n",
      "True is type: <class 'bool'>\n",
      "None is type: <class 'NoneType'>\n"
     ]
    }
   ],
   "source": [
    "# Exploring Data Types\n",
    "my_int = 42\n",
    "my_float = 3.14\n",
    "my_string = \"Hello Python!\"\n",
    "my_bool = True\n",
    "my_none = None\n",
    "\n",
    "# Use type() to check\n",
    "print(f\"{my_int} is type: {type(my_int)}\")\n",
    "print(f\"{my_float} is type: {type(my_float)}\")\n",
    "print(f\"{my_string} is type: {type(my_string)}\")\n",
    "print(f\"{my_bool} is type: {type(my_bool)}\")\n",
    "print(f\"{my_none} is type: {type(my_none)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "118962d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'123' + 1 = 124\n",
      "I am 14 years old\n",
      "$19.99 rounded down is $19\n"
     ]
    }
   ],
   "source": [
    "# Type Conversion (Casting)\n",
    "text_number = \"123\"\n",
    "real_number = int(text_number)\n",
    "print(f\"'{text_number}' + 1 = {real_number + 1}\")\n",
    "\n",
    "age = 14\n",
    "age_text = str(age)\n",
    "print(f\"I am \" + age_text + \" years old\")\n",
    "\n",
    "price = 19.99\n",
    "whole_price = int(price)  # Removes decimal\n",
    "print(f\"${price} rounded down is ${whole_price}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715251bd",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2: Data Structures ðŸ“¦\n",
    "\n",
    "## 2.1 Lists\n",
    "An ordered collection you CAN modify. Like a shopping list!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "51a4c441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fruits: ['apple', 'banana', 'cherry']\n",
      "Numbers: [1, 2, 3, 4, 5]\n",
      "\n",
      "First fruit: apple\n",
      "Last fruit: cherry\n"
     ]
    }
   ],
   "source": [
    "# Creating Lists\n",
    "fruits = [\"apple\", \"banana\", \"cherry\"]\n",
    "numbers = [1, 2, 3, 4, 5]\n",
    "mixed = [\"hello\", 42, 3.14, True]\n",
    "\n",
    "print(\"Fruits:\", fruits)\n",
    "print(\"Numbers:\", numbers)\n",
    "\n",
    "# Accessing by INDEX (starts at 0!)\n",
    "print(f\"\\nFirst fruit: {fruits[0]}\")\n",
    "print(f\"Last fruit: {fruits[-1]}\")  # Negative = from end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "6ab0e466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: ['red', 'green', 'blue']\n",
      "After append: ['red', 'green', 'blue', 'yellow']\n",
      "After insert: ['red', 'orange', 'green', 'blue', 'yellow']\n",
      "After remove: ['red', 'orange', 'blue', 'yellow']\n",
      "Popped: yellow, Now: ['red', 'orange', 'blue']\n",
      "After change: ['purple', 'orange', 'blue']\n",
      "\n",
      "Length: 3\n"
     ]
    }
   ],
   "source": [
    "# Modifying Lists\n",
    "colors = [\"red\", \"green\", \"blue\"]\n",
    "print(\"Original:\", colors)\n",
    "\n",
    "colors.append(\"yellow\")      # Add to end\n",
    "print(\"After append:\", colors)\n",
    "\n",
    "colors.insert(1, \"orange\")   # Insert at position\n",
    "print(\"After insert:\", colors)\n",
    "\n",
    "colors.remove(\"green\")       # Remove by value\n",
    "print(\"After remove:\", colors)\n",
    "\n",
    "popped = colors.pop()        # Remove & return last\n",
    "print(f\"Popped: {popped}, Now: {colors}\")\n",
    "\n",
    "colors[0] = \"purple\"         # Change item\n",
    "print(\"After change:\", colors)\n",
    "\n",
    "print(f\"\\nLength: {len(colors)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "b6b5ca1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My favorite games:\n",
      "  - Minecraft\n",
      "  - Fortnite\n",
      "  - Roblox\n",
      "  - Among Us\n",
      "\n",
      "With rankings:\n",
      "  #1: Minecraft\n",
      "  #2: Fortnite\n",
      "  #3: Roblox\n",
      "  #4: Among Us\n",
      "\n",
      "First 3: [0, 1, 2]\n",
      "Last 3: [7, 8, 9]\n",
      "Middle: [3, 4, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "# Looping through Lists\n",
    "games = [\"Minecraft\", \"Fortnite\", \"Roblox\", \"Among Us\"]\n",
    "\n",
    "print(\"My favorite games:\")\n",
    "for game in games:\n",
    "    print(f\"  - {game}\")\n",
    "\n",
    "# With index using enumerate()\n",
    "print(\"\\nWith rankings:\")\n",
    "for index, game in enumerate(games, start=1):\n",
    "    print(f\"  #{index}: {game}\")\n",
    "\n",
    "# Slicing\n",
    "numbers = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "print(f\"\\nFirst 3: {numbers[:3]}\")\n",
    "print(f\"Last 3: {numbers[-3:]}\")\n",
    "print(f\"Middle: {numbers[3:7]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1183623",
   "metadata": {},
   "source": [
    "## 2.2 Tuples\n",
    "Like lists but CANNOT modify after creation. \"Immutable\". Good for coordinates, RGB colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "15923728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coordinates: (10, 20)\n",
      "Red RGB: (255, 0, 0)\n",
      "Person: ('Alice', 14, '9th Grade')\n",
      "\n",
      "X: 10, Y: 20\n",
      "\n",
      "Unpacked: x=10, y=20\n",
      "Alice is 14 years old in 9th Grade\n"
     ]
    }
   ],
   "source": [
    "# Creating Tuples - use parentheses ()\n",
    "coordinates = (10, 20)\n",
    "rgb_red = (255, 0, 0)\n",
    "person = (\"Alice\", 14, \"9th Grade\")\n",
    "\n",
    "print(f\"Coordinates: {coordinates}\")\n",
    "print(f\"Red RGB: {rgb_red}\")\n",
    "print(f\"Person: {person}\")\n",
    "\n",
    "# Accessing items\n",
    "print(f\"\\nX: {coordinates[0]}, Y: {coordinates[1]}\")\n",
    "\n",
    "# Tuple unpacking\n",
    "x, y = coordinates\n",
    "name, age, grade = person\n",
    "print(f\"\\nUnpacked: x={x}, y={y}\")\n",
    "print(f\"{name} is {age} years old in {grade}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78314df",
   "metadata": {},
   "source": [
    "## 2.3 Dictionaries\n",
    "Stores **key-value pairs**. Like a real dictionary: key = word, value = definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "1e73b986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student: {'name': 'Alex', 'age': 14, 'grade': 9, 'gpa': 3.8}\n",
      "\n",
      "Name: Alex\n",
      "GPA: 3.8\n",
      "Color: Not specified\n"
     ]
    }
   ],
   "source": [
    "# Creating Dictionaries - use curly braces {}\n",
    "student = {\n",
    "    \"name\": \"Alex\",\n",
    "    \"age\": 14,\n",
    "    \"grade\": 9,\n",
    "    \"gpa\": 3.8\n",
    "}\n",
    "\n",
    "print(\"Student:\", student)\n",
    "\n",
    "# Accessing by KEY\n",
    "print(f\"\\nName: {student['name']}\")\n",
    "print(f\"GPA: {student['gpa']}\")\n",
    "\n",
    "# Using .get() is safer\n",
    "print(f\"Color: {student.get('color', 'Not specified')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "872340c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report Card:\n",
      "  Math: 95 (Pass)\n",
      "  English: 88 (Pass)\n",
      "  Science: 92 (Pass)\n",
      "  History: 85 (Pass)\n",
      "\n",
      "Average: 90.0\n"
     ]
    }
   ],
   "source": [
    "# Looping through Dictionaries\n",
    "grades = {\"Math\": 95, \"English\": 88, \"Science\": 92, \"History\": 85}\n",
    "\n",
    "print(\"Report Card:\")\n",
    "for subject, score in grades.items():\n",
    "    status = \"Pass\" if score >= 60 else \"Fail\"\n",
    "    print(f\"  {subject}: {score} ({status})\")\n",
    "\n",
    "average = sum(grades.values()) / len(grades)\n",
    "print(f\"\\nAverage: {average:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e69026",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 3: Functions and Classes ðŸ”§\n",
    "\n",
    "## 3.1 Functions\n",
    "A reusable block of code. Like a recipe: ingredients (parameters) â†’ steps â†’ dish (return value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "b5a9b067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, World!\n",
      "Hello, World!\n"
     ]
    }
   ],
   "source": [
    "def say_hello():\n",
    "    \"\"\"\n",
    "    Print a simple greeting message to the console.\n",
    "    \n",
    "    This function demonstrates the most basic form of a Python function\n",
    "    with no parameters and no return value.\n",
    "    \n",
    "    Parameters:\n",
    "        None\n",
    "    \n",
    "    Returns:\n",
    "        None: This function only prints output and returns nothing.\n",
    "    \n",
    "    Example:\n",
    "        >>> say_hello()\n",
    "        Hello, World!\n",
    "    \"\"\"\n",
    "    print(\"Hello, World!\")\n",
    "\n",
    "# Call the function\n",
    "say_hello()\n",
    "say_hello()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "25cc18a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, Alice! Nice to meet you!\n",
      "Hello, Bob! Nice to meet you!\n"
     ]
    }
   ],
   "source": [
    "def greet(name):\n",
    "    \"\"\"\n",
    "    Print a personalized greeting for a specific person.\n",
    "    \n",
    "    This function demonstrates a function with one parameter that\n",
    "    customizes the output based on the input provided.\n",
    "    \n",
    "    Parameters:\n",
    "        name (str): The name of the person to greet. This will be\n",
    "            included in the greeting message displayed to the user.\n",
    "    \n",
    "    Returns:\n",
    "        None: This function only prints output and returns nothing.\n",
    "    \n",
    "    Example:\n",
    "        >>> greet(\"Alice\")\n",
    "        Hello, Alice! Nice to meet you!\n",
    "    \"\"\"\n",
    "    print(f\"Hello, {name}! Nice to meet you!\")\n",
    "\n",
    "greet(\"Alice\")\n",
    "greet(\"Bob\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "ca173535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 + 3 = 8\n"
     ]
    }
   ],
   "source": [
    "def add(a, b):\n",
    "    \"\"\"\n",
    "    Add two numbers together and return the sum.\n",
    "    \n",
    "    This function demonstrates a function with multiple parameters\n",
    "    and a return value that can be used in further calculations.\n",
    "    \n",
    "    Parameters:\n",
    "        a (int or float): The first number to add. Can be any numeric type\n",
    "            that supports the addition operation.\n",
    "        b (int or float): The second number to add. Can be any numeric type\n",
    "            that supports the addition operation.\n",
    "    \n",
    "    Returns:\n",
    "        int or float: The sum of a and b. The return type matches the\n",
    "            input types (int + int = int, float involved = float).\n",
    "    \n",
    "    Example:\n",
    "        >>> result = add(5, 3)\n",
    "        >>> print(result)\n",
    "        8\n",
    "    \"\"\"\n",
    "    result = a + b\n",
    "    return result\n",
    "\n",
    "sum_result = add(5, 3)\n",
    "print(f\"5 + 3 = {sum_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "24bedf7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 squared: 9\n",
      "3 cubed: 27\n",
      "2 to the 10th: 1024\n"
     ]
    }
   ],
   "source": [
    "def power(base, exponent=2):\n",
    "    \"\"\"\n",
    "    Raise a base number to a given exponent power.\n",
    "    \n",
    "    This function demonstrates default parameter values. If no exponent\n",
    "    is provided, it defaults to 2 (squaring the base).\n",
    "    \n",
    "    Parameters:\n",
    "        base (int or float): The base number to be raised to a power.\n",
    "            This is the number that will be multiplied by itself.\n",
    "        exponent (int or float, optional): The power to raise the base to.\n",
    "            Defaults to 2, which squares the number. Can be any numeric value.\n",
    "    \n",
    "    Returns:\n",
    "        int or float: The result of base raised to the exponent power.\n",
    "            Calculated as base ** exponent.\n",
    "    \n",
    "    Example:\n",
    "        >>> power(3)       # 3 squared = 9\n",
    "        9\n",
    "        >>> power(2, 10)   # 2 to the 10th = 1024\n",
    "        1024\n",
    "    \"\"\"\n",
    "    return base ** exponent\n",
    "\n",
    "print(f\"3 squared: {power(3)}\")\n",
    "print(f\"3 cubed: {power(3, 3)}\")\n",
    "print(f\"2 to the 10th: {power(2, 10)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "fa992e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List: [4, 1, 9, 2, 7, 3]\n",
      "Min: 1, Max: 9\n"
     ]
    }
   ],
   "source": [
    "def get_min_max(numbers):\n",
    "    \"\"\"\n",
    "    Find and return both the minimum and maximum values in a list.\n",
    "    \n",
    "    This function demonstrates returning multiple values from a function\n",
    "    using a tuple, which can then be unpacked by the caller.\n",
    "    \n",
    "    Parameters:\n",
    "        numbers (list): A list of numeric values (int or float) to search\n",
    "            through. The list must contain at least one element.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: A tuple containing two elements:\n",
    "            - minimum_value: The smallest number in the list\n",
    "            - maximum_value: The largest number in the list\n",
    "    \n",
    "    Example:\n",
    "        >>> min_val, max_val = get_min_max([4, 1, 9, 2])\n",
    "        >>> print(f\"Min: {min_val}, Max: {max_val}\")\n",
    "        Min: 1, Max: 9\n",
    "    \"\"\"\n",
    "    return min(numbers), max(numbers)\n",
    "\n",
    "data = [4, 1, 9, 2, 7, 3]\n",
    "minimum, maximum = get_min_max(data)\n",
    "print(f\"List: {data}\")\n",
    "print(f\"Min: {minimum}, Max: {maximum}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a26cc3",
   "metadata": {},
   "source": [
    "## 3.2 Classes and Objects ðŸ—ï¸\n",
    "\n",
    "A **class** is a blueprint. An **object** is built from that blueprint.\n",
    "- Class = Blueprint for a house\n",
    "- Object = An actual house\n",
    "\n",
    "Classes bundle **data** (attributes) and **behavior** (methods) together!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "19de0c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Dog 1 ===\n",
      "Name: Buddy, Breed: Golden Retriever, Age: 3\n",
      "Buddy says: Woof! Woof!\n",
      "Buddy is playing! Energy: 80\n",
      "\n",
      "=== Dog 2 ===\n",
      "Name: Max, Breed: German Shepherd, Age: 5\n",
      "Max says: Woof! Woof!\n"
     ]
    }
   ],
   "source": [
    "class Dog:\n",
    "    \"\"\"\n",
    "    A class representing a dog with attributes and behaviors.\n",
    "    \n",
    "    This class demonstrates object-oriented programming concepts including\n",
    "    instance attributes, methods, and state management. Each Dog instance\n",
    "    has its own name, breed, age, and energy level.\n",
    "    \n",
    "    Attributes:\n",
    "        name (str): The dog's name, set during initialization.\n",
    "        breed (str): The dog's breed (e.g., \"Golden Retriever\").\n",
    "        age (int): The dog's age in years.\n",
    "        energy (int): The dog's current energy level, ranging from 0 to 100.\n",
    "            Starts at 100 and decreases when playing.\n",
    "    \n",
    "    Example:\n",
    "        >>> my_dog = Dog(\"Buddy\", \"Golden Retriever\", 3)\n",
    "        >>> my_dog.bark()\n",
    "        Buddy says: Woof! Woof!\n",
    "        >>> my_dog.play()\n",
    "        Buddy is playing! Energy: 80\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, name, breed, age):\n",
    "        \"\"\"\n",
    "        Initialize a new Dog instance with the given attributes.\n",
    "        \n",
    "        This is the constructor method that runs when a new Dog object\n",
    "        is created. It sets up the initial state of the dog.\n",
    "        \n",
    "        Parameters:\n",
    "            name (str): The dog's name.\n",
    "            breed (str): The dog's breed.\n",
    "            age (int): The dog's age in years.\n",
    "        \"\"\"\n",
    "        self.name = name\n",
    "        self.breed = breed\n",
    "        self.age = age\n",
    "        self.energy = 100\n",
    "    \n",
    "    def bark(self):\n",
    "        \"\"\"\n",
    "        Make the dog bark by printing a bark message.\n",
    "        \n",
    "        This method simulates the dog barking by printing a message\n",
    "        that includes the dog's name.\n",
    "        \n",
    "        Returns:\n",
    "            None: Only prints output.\n",
    "        \"\"\"\n",
    "        print(f\"{self.name} says: Woof! Woof!\")\n",
    "    \n",
    "    def play(self):\n",
    "        \"\"\"\n",
    "        Make the dog play, consuming 20 energy points.\n",
    "        \n",
    "        If the dog has enough energy (>= 20), it plays and loses 20 energy.\n",
    "        If the dog is too tired (energy < 20), it refuses to play.\n",
    "        \n",
    "        Returns:\n",
    "            None: Only prints output and modifies energy.\n",
    "        \"\"\"\n",
    "        if self.energy >= 20:\n",
    "            self.energy -= 20\n",
    "            print(f\"{self.name} is playing! Energy: {self.energy}\")\n",
    "        else:\n",
    "            print(f\"{self.name} is too tired to play!\")\n",
    "    \n",
    "    def sleep(self):\n",
    "        \"\"\"\n",
    "        Restore the dog's energy to full (100 points).\n",
    "        \n",
    "        This method simulates the dog sleeping and recovering all\n",
    "        of its energy back to the maximum of 100.\n",
    "        \n",
    "        Returns:\n",
    "            None: Only prints output and modifies energy.\n",
    "        \"\"\"\n",
    "        self.energy = 100\n",
    "        print(f\"{self.name} slept and recovered energy!\")\n",
    "    \n",
    "    def info(self):\n",
    "        \"\"\"\n",
    "        Display the dog's information.\n",
    "        \n",
    "        Prints a formatted string showing the dog's name, breed, and age.\n",
    "        \n",
    "        Returns:\n",
    "            None: Only prints output.\n",
    "        \"\"\"\n",
    "        print(f\"Name: {self.name}, Breed: {self.breed}, Age: {self.age}\")\n",
    "\n",
    "# Create objects\n",
    "dog1 = Dog(\"Buddy\", \"Golden Retriever\", 3)\n",
    "dog2 = Dog(\"Max\", \"German Shepherd\", 5)\n",
    "\n",
    "print(\"=== Dog 1 ===\")\n",
    "dog1.info()\n",
    "dog1.bark()\n",
    "dog1.play()\n",
    "\n",
    "print(\"\\n=== Dog 2 ===\")\n",
    "dog2.info()\n",
    "dog2.bark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "8e9793f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deposited $50. New balance: $150\n",
      "Withdrew $30. New balance: $120\n",
      "Insufficient funds! Balance: $120\n",
      "Deposited $100. New balance: $220\n",
      "\n",
      "Alex's Transaction History:\n",
      "  +$50\n",
      "  -$30\n",
      "  +$100\n",
      "Current Balance: $220\n"
     ]
    }
   ],
   "source": [
    "class BankAccount:\n",
    "    \"\"\"\n",
    "    A class representing a simple bank account with deposit/withdraw operations.\n",
    "    \n",
    "    This class demonstrates encapsulation by managing account balance and\n",
    "    transaction history. It includes validation to prevent invalid operations\n",
    "    like withdrawing more than the available balance.\n",
    "    \n",
    "    Attributes:\n",
    "        owner (str): The name of the account owner.\n",
    "        balance (float): Current account balance in dollars.\n",
    "        transactions (list): History of all transactions as formatted strings.\n",
    "    \n",
    "    Example:\n",
    "        >>> account = BankAccount(\"Alex\", 100)\n",
    "        >>> account.deposit(50)\n",
    "        Deposited $50. New balance: $150\n",
    "        >>> account.withdraw(30)\n",
    "        Withdrew $30. New balance: $120\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, owner, initial_balance=0):\n",
    "        \"\"\"\n",
    "        Initialize a new bank account.\n",
    "        \n",
    "        Creates a new account with the specified owner and optional\n",
    "        starting balance. Also initializes an empty transaction history.\n",
    "        \n",
    "        Parameters:\n",
    "            owner (str): The name of the account owner.\n",
    "            initial_balance (float, optional): Starting balance. Defaults to 0.\n",
    "        \"\"\"\n",
    "        self.owner = owner\n",
    "        self.balance = initial_balance\n",
    "        self.transactions = []\n",
    "    \n",
    "    def deposit(self, amount):\n",
    "        \"\"\"\n",
    "        Add money to the account.\n",
    "        \n",
    "        Validates that the deposit amount is positive before adding\n",
    "        to the balance and recording the transaction.\n",
    "        \n",
    "        Parameters:\n",
    "            amount (float): The amount to deposit. Must be positive.\n",
    "        \n",
    "        Returns:\n",
    "            None: Prints confirmation or error message.\n",
    "        \"\"\"\n",
    "        if amount > 0:\n",
    "            self.balance += amount\n",
    "            self.transactions.append(f\"+${amount}\")\n",
    "            print(f\"Deposited ${amount}. New balance: ${self.balance}\")\n",
    "        else:\n",
    "            print(\"Deposit amount must be positive!\")\n",
    "    \n",
    "    def withdraw(self, amount):\n",
    "        \"\"\"\n",
    "        Remove money from the account.\n",
    "        \n",
    "        Validates that the withdrawal amount is positive and doesn't\n",
    "        exceed the current balance before processing.\n",
    "        \n",
    "        Parameters:\n",
    "            amount (float): The amount to withdraw. Must be positive\n",
    "                and not exceed the current balance.\n",
    "        \n",
    "        Returns:\n",
    "            None: Prints confirmation or error message.\n",
    "        \"\"\"\n",
    "        if amount > self.balance:\n",
    "            print(f\"Insufficient funds! Balance: ${self.balance}\")\n",
    "        elif amount <= 0:\n",
    "            print(\"Withdrawal amount must be positive!\")\n",
    "        else:\n",
    "            self.balance -= amount\n",
    "            self.transactions.append(f\"-${amount}\")\n",
    "            print(f\"Withdrew ${amount}. New balance: ${self.balance}\")\n",
    "    \n",
    "    def get_balance(self):\n",
    "        \"\"\"\n",
    "        Return the current account balance.\n",
    "        \n",
    "        Returns:\n",
    "            float: The current balance in the account.\n",
    "        \"\"\"\n",
    "        return self.balance\n",
    "    \n",
    "    def show_history(self):\n",
    "        \"\"\"\n",
    "        Display all transaction history and current balance.\n",
    "        \n",
    "        Prints a formatted list of all deposits and withdrawals,\n",
    "        followed by the current account balance.\n",
    "        \n",
    "        Returns:\n",
    "            None: Only prints output.\n",
    "        \"\"\"\n",
    "        print(f\"\\n{self.owner}'s Transaction History:\")\n",
    "        for t in self.transactions:\n",
    "            print(f\"  {t}\")\n",
    "        print(f\"Current Balance: ${self.balance}\")\n",
    "\n",
    "# Test the BankAccount\n",
    "account = BankAccount(\"Alex\", 100)\n",
    "account.deposit(50)\n",
    "account.withdraw(30)\n",
    "account.withdraw(200)  # Should fail\n",
    "account.deposit(100)\n",
    "account.show_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fc1f2f",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 4: Neural Network from Scratch! ðŸ§ \n",
    "\n",
    "## How Neural Networks Learn:\n",
    "1. **Forward Pass** - Data goes through, makes predictions\n",
    "2. **Loss Calculation** - Measure how wrong we are\n",
    "3. **Backward Pass (Backpropagation)** - Calculate how to fix weights\n",
    "4. **Optimization** - Actually update the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "f8001eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy ready! Let's build a neural network!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "print(\"NumPy ready! Let's build a neural network!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "a3cdbaf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid examples:\n",
      "sigmoid(-10) = 0.000045  (near 0)\n",
      "sigmoid(0)   = 0.500000  (exactly 0.5)\n",
      "sigmoid(10)  = 0.999955  (near 1)\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Sigmoid activation function that squishes any number to range (0, 1).\n",
    "    \n",
    "    The sigmoid function is one of the most common activation functions\n",
    "    in neural networks. It's especially useful for binary classification\n",
    "    because it outputs values that can be interpreted as probabilities.\n",
    "    \n",
    "    Parameters:\n",
    "        x (numpy.ndarray or float): Input value(s) to apply sigmoid to.\n",
    "            Can be a single number or an array of any shape.\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray or float: Output value(s) in the range (0, 1).\n",
    "            Large positive inputs close to 1\n",
    "            Large negative inputs close to 0\n",
    "            Zero exactly 0.5\n",
    "    \n",
    "    Formula:\n",
    "        sigmoid(x) = 1 / (1 + e^(-x))\n",
    "    \n",
    "    Example:\n",
    "        >>> sigmoid(0)\n",
    "        0.5\n",
    "        >>> sigmoid(10)\n",
    "        0.9999546...\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    \"\"\"\n",
    "    Compute the derivative of the sigmoid function.\n",
    "    \n",
    "    This derivative is essential for backpropagation, as it tells us\n",
    "    how much the sigmoid output changes when the input changes.\n",
    "    The derivative has a nice property: it can be computed from\n",
    "    the sigmoid output itself.\n",
    "    \n",
    "    Parameters:\n",
    "        x (numpy.ndarray or float): Input value(s) to compute derivative at.\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray or float: The gradient/derivative of sigmoid at x.\n",
    "            Maximum value of 0.25 occurs at x=0.\n",
    "    \n",
    "    Formula:\n",
    "        sigmoid_derivative(x) = sigmoid(x) * (1 - sigmoid(x))\n",
    "    \n",
    "    Note:\n",
    "        Used in backpropagation to calculate gradients for weight updates.\n",
    "    \"\"\"\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "\n",
    "# Visualize\n",
    "x = np.linspace(-10, 10, 100)\n",
    "y = sigmoid(x)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(x, y, 'b-', linewidth=2)\n",
    "plt.title('Sigmoid Function', fontsize=14)\n",
    "plt.xlabel('Input (x)')\n",
    "plt.ylabel('Output (0 to 1)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axhline(y=0.5, color='r', linestyle='--', alpha=0.5)\n",
    "plt.show()\n",
    "\n",
    "print(\"Sigmoid examples:\")\n",
    "print(f\"sigmoid(-10) = {sigmoid(-10):.6f}  (near 0)\")\n",
    "print(f\"sigmoid(0)   = {sigmoid(0):.6f}  (exactly 0.5)\")\n",
    "print(f\"sigmoid(10)  = {sigmoid(10):.6f}  (near 1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b48566",
   "metadata": {},
   "source": [
    "## The XOR Problem\n",
    "\n",
    "| Input A | Input B | Output |\n",
    "|---------|---------|--------|\n",
    "| 0 | 0 | 0 |\n",
    "| 0 | 1 | 1 |\n",
    "| 1 | 0 | 1 |\n",
    "| 1 | 1 | 0 |\n",
    "\n",
    "Output is 1 when inputs are DIFFERENT, 0 when SAME."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "230414c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XOR Dataset:\n",
      "Input: [0 0] â†’ Output: 0\n",
      "Input: [0 1] â†’ Output: 1\n",
      "Input: [1 0] â†’ Output: 1\n",
      "Input: [1 1] â†’ Output: 0\n",
      "\n",
      "Network: 2 inputs â†’ 4 hidden â†’ 1 output\n"
     ]
    }
   ],
   "source": [
    "# XOR Dataset\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "print(\"XOR Dataset:\")\n",
    "for i in range(len(X)):\n",
    "    print(f\"Input: {X[i]} â†’ Output: {y[i][0]}\")\n",
    "\n",
    "# Initialize Network\n",
    "np.random.seed(42)\n",
    "W1 = np.random.randn(2, 4) * 0.5\n",
    "b1 = np.zeros((1, 4))\n",
    "W2 = np.random.randn(4, 1) * 0.5\n",
    "b2 = np.zeros((1, 1))\n",
    "print(f\"\\nNetwork: 2 inputs â†’ 4 hidden â†’ 1 output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "e94779df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE Training:\n",
      "Input: [0 0] -> Pred: 0.4467 | Expected: 0\n",
      "Input: [0 1] -> Pred: 0.4303 | Expected: 1\n",
      "Input: [1 0] -> Pred: 0.4270 | Expected: 1\n",
      "Input: [1 1] -> Pred: 0.4126 | Expected: 0\n",
      "\n",
      "Loss: 0.2557\n"
     ]
    }
   ],
   "source": [
    "def forward(X, W1, b1, W2, b2):\n",
    "    \"\"\"\n",
    "    Perform forward pass through a 2-layer neural network.\n",
    "    \n",
    "    The forward pass computes predictions by passing input data through\n",
    "    the network layers. Each layer applies a linear transformation\n",
    "    (weights * inputs + bias) followed by the sigmoid activation function.\n",
    "    \n",
    "    Parameters:\n",
    "        X (numpy.ndarray): Input data of shape (num_samples, num_features).\n",
    "            Each row is one sample, each column is one feature.\n",
    "        W1 (numpy.ndarray): Weights for layer 1, shape (num_features, num_hidden).\n",
    "            Transforms input features to hidden layer neurons.\n",
    "        b1 (numpy.ndarray): Bias for layer 1, shape (1, num_hidden).\n",
    "            Added to each hidden neuron's computation.\n",
    "        W2 (numpy.ndarray): Weights for layer 2, shape (num_hidden, num_output).\n",
    "            Transforms hidden neurons to output.\n",
    "        b2 (numpy.ndarray): Bias for layer 2, shape (1, num_output).\n",
    "            Added to each output neuron's computation.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - a2 (numpy.ndarray): Output predictions, shape (num_samples, num_output).\n",
    "            - cache (dict): Dictionary containing intermediate values needed for\n",
    "              backpropagation: 'z1', 'a1', 'z2', 'a2'.\n",
    "    \n",
    "    Process:\n",
    "        1. z1 = X @ W1 + b1  (linear transformation)\n",
    "        2. a1 = sigmoid(z1)  (activation)\n",
    "        3. z2 = a1 @ W2 + b2 (linear transformation)\n",
    "        4. a2 = sigmoid(z2)  (activation/output)\n",
    "    \"\"\"\n",
    "    z1 = np.dot(X, W1) + b1\n",
    "    a1 = sigmoid(z1)\n",
    "    z2 = np.dot(a1, W2) + b2\n",
    "    a2 = sigmoid(z2)\n",
    "    cache = {'z1': z1, 'a1': a1, 'z2': z2, 'a2': a2}\n",
    "    return a2, cache\n",
    "\n",
    "\n",
    "def mse_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate Mean Squared Error loss between true and predicted values.\n",
    "    \n",
    "    MSE is one of the most common loss functions for regression problems.\n",
    "    It measures the average squared difference between predictions and\n",
    "    actual values, penalizing larger errors more heavily.\n",
    "    \n",
    "    Parameters:\n",
    "        y_true (numpy.ndarray): True target values (ground truth).\n",
    "        y_pred (numpy.ndarray): Predicted values from the model.\n",
    "    \n",
    "    Returns:\n",
    "        float: The mean squared error, always non-negative.\n",
    "            Lower values indicate better predictions.\n",
    "    \n",
    "    Formula:\n",
    "        MSE = (1/n) * sum((y_true - y_pred)^2)\n",
    "    \"\"\"\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "\n",
    "# Test forward pass BEFORE training\n",
    "predictions, cache = forward(X, W1, b1, W2, b2)\n",
    "print(\"BEFORE Training:\")\n",
    "for i in range(len(X)):\n",
    "    print(f\"Input: {X[i]} -> Pred: {predictions[i][0]:.4f} | Expected: {y[i][0]}\")\n",
    "print(f\"\\nLoss: {mse_loss(y, predictions):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "db9df9be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backward pass and update functions ready!\n"
     ]
    }
   ],
   "source": [
    "def backward(X, y, cache, W2):\n",
    "    \"\"\"\n",
    "    Perform backward pass (backpropagation) to calculate gradients.\n",
    "    \n",
    "    Backpropagation computes how much each weight contributed to the error,\n",
    "    allowing us to update weights in the direction that reduces error.\n",
    "    Uses the chain rule to propagate gradients from output to input.\n",
    "    \n",
    "    Parameters:\n",
    "        X (numpy.ndarray): Input data used in forward pass.\n",
    "        y (numpy.ndarray): True target labels.\n",
    "        cache (dict): Dictionary containing forward pass intermediate values:\n",
    "            - 'z1': Pre-activation values for hidden layer\n",
    "            - 'a1': Post-activation values for hidden layer\n",
    "            - 'z2': Pre-activation values for output layer\n",
    "            - 'a2': Post-activation values (predictions)\n",
    "        W2 (numpy.ndarray): Weights for layer 2, needed for gradient computation.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing gradients for all parameters:\n",
    "            - 'dW1': Gradient for W1, shape matches W1\n",
    "            - 'db1': Gradient for b1, shape matches b1\n",
    "            - 'dW2': Gradient for W2, shape matches W2\n",
    "            - 'db2': Gradient for b2, shape matches b2\n",
    "    \n",
    "    Process:\n",
    "        1. Compute output layer error: dz2 = a2 - y\n",
    "        2. Compute W2 gradient: dW2 = a1.T @ dz2 / m\n",
    "        3. Propagate error to hidden layer: da1 = dz2 @ W2.T\n",
    "        4. Compute hidden layer error: dz1 = da1 * sigmoid'(z1)\n",
    "        5. Compute W1 gradient: dW1 = X.T @ dz1 / m\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    a1, a2, z1 = cache['a1'], cache['a2'], cache['z1']\n",
    "    \n",
    "    # Output layer gradients\n",
    "    dz2 = a2 - y\n",
    "    dW2 = np.dot(a1.T, dz2) / m\n",
    "    db2 = np.sum(dz2, axis=0, keepdims=True) / m\n",
    "    \n",
    "    # Hidden layer gradients\n",
    "    da1 = np.dot(dz2, W2.T)\n",
    "    dz1 = da1 * sigmoid_derivative(z1)\n",
    "    dW1 = np.dot(X.T, dz1) / m\n",
    "    db1 = np.sum(dz1, axis=0, keepdims=True) / m\n",
    "    \n",
    "    return {'dW1': dW1, 'db1': db1, 'dW2': dW2, 'db2': db2}\n",
    "\n",
    "\n",
    "def update_weights(W1, b1, W2, b2, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update neural network weights using gradient descent.\n",
    "    \n",
    "    Gradient descent moves each weight in the opposite direction of its\n",
    "    gradient, scaled by the learning rate. This reduces the loss over time.\n",
    "    \n",
    "    Parameters:\n",
    "        W1 (numpy.ndarray): Current weights for layer 1.\n",
    "        b1 (numpy.ndarray): Current bias for layer 1.\n",
    "        W2 (numpy.ndarray): Current weights for layer 2.\n",
    "        b2 (numpy.ndarray): Current bias for layer 2.\n",
    "        grads (dict): Dictionary containing gradients from backward pass.\n",
    "        learning_rate (float): Step size for weight updates. Larger values\n",
    "            mean bigger steps but may overshoot; smaller values are more\n",
    "            stable but slower.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Updated parameters (W1, b1, W2, b2) after gradient descent step.\n",
    "    \n",
    "    Formula:\n",
    "        W_new = W_old - learning_rate * gradient\n",
    "    \"\"\"\n",
    "    W1 = W1 - learning_rate * grads['dW1']\n",
    "    b1 = b1 - learning_rate * grads['db1']\n",
    "    W2 = W2 - learning_rate * grads['dW2']\n",
    "    b2 = b2 - learning_rate * grads['db2']\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "print(\"Backward pass and update functions ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "0fdd171a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Training Started!\n",
      "Epoch     0 | Loss: 0.255675\n",
      "Epoch  2000 | Loss: 0.000005\n",
      "Epoch  4000 | Loss: 0.000001\n",
      "Epoch  6000 | Loss: 0.000000\n",
      "Epoch  8000 | Loss: 0.000000\n",
      "âœ… Training Complete!\n"
     ]
    }
   ],
   "source": [
    "def train_network(X, y, epochs=10000, learning_rate=2.0):\n",
    "    \"\"\"\n",
    "    Train a 2-layer neural network from scratch using gradient descent.\n",
    "    \n",
    "    This function implements the complete training loop: forward pass to make\n",
    "    predictions, loss calculation, backward pass to compute gradients, and\n",
    "    weight updates to improve the model.\n",
    "    \n",
    "    Parameters:\n",
    "        X (numpy.ndarray): Input features of shape (num_samples, num_features).\n",
    "        y (numpy.ndarray): Target labels of shape (num_samples, num_outputs).\n",
    "        epochs (int, optional): Number of complete passes through the training\n",
    "            data. More epochs = more learning but also more time. Defaults to 10000.\n",
    "        learning_rate (float, optional): Step size for gradient descent updates.\n",
    "            Controls how much weights change each iteration. Defaults to 2.0.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - W1 (numpy.ndarray): Trained weights for layer 1.\n",
    "            - b1 (numpy.ndarray): Trained bias for layer 1.\n",
    "            - W2 (numpy.ndarray): Trained weights for layer 2.\n",
    "            - b2 (numpy.ndarray): Trained bias for layer 2.\n",
    "            - losses (list): Loss value at each epoch for visualization.\n",
    "    \n",
    "    Training Loop:\n",
    "        For each epoch:\n",
    "        1. Forward pass: Compute predictions\n",
    "        2. Calculate loss: Measure prediction error\n",
    "        3. Backward pass: Compute gradients\n",
    "        4. Update weights: Apply gradient descent\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    W1 = np.random.randn(2, 4) * 0.5\n",
    "    b1 = np.zeros((1, 4))\n",
    "    W2 = np.random.randn(4, 1) * 0.5\n",
    "    b2 = np.zeros((1, 1))\n",
    "    losses = []\n",
    "    \n",
    "    print(\"ðŸš€ Training Started!\")\n",
    "    for epoch in range(epochs):\n",
    "        preds, cache = forward(X, W1, b1, W2, b2)\n",
    "        loss = mse_loss(y, preds)\n",
    "        losses.append(loss)\n",
    "        grads = backward(X, y, cache, W2)\n",
    "        W1, b1, W2, b2 = update_weights(W1, b1, W2, b2, grads, learning_rate)\n",
    "        \n",
    "        if epoch % 2000 == 0:\n",
    "            print(f\"Epoch {epoch:5d} | Loss: {loss:.6f}\")\n",
    "    \n",
    "    print(\"âœ… Training Complete!\")\n",
    "    return W1, b1, W2, b2, losses\n",
    "\n",
    "W1_t, b1_t, W2_t, b2_t, loss_history = train_network(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "57a1f183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¯ AFTER Training:\n",
      "Input: [0 0] â†’ Pred: 0.0005 (â‰ˆ0) | Expected: 0 âœ…\n",
      "Input: [0 1] â†’ Pred: 0.9997 (â‰ˆ1) | Expected: 1 âœ…\n",
      "Input: [1 0] â†’ Pred: 0.9997 (â‰ˆ1) | Expected: 1 âœ…\n",
      "Input: [1 1] â†’ Pred: 0.0003 (â‰ˆ0) | Expected: 0 âœ…\n",
      "\n",
      "ðŸŽ‰ Accuracy: 100%\n"
     ]
    }
   ],
   "source": [
    "# Test trained network\n",
    "print(\"ðŸŽ¯ AFTER Training:\")\n",
    "final_preds, _ = forward(X, W1_t, b1_t, W2_t, b2_t)\n",
    "\n",
    "for i in range(len(X)):\n",
    "    pred = final_preds[i][0]\n",
    "    expected = y[i][0]\n",
    "    rounded = round(pred)\n",
    "    status = \"âœ…\" if rounded == expected else \"âŒ\"\n",
    "    print(f\"Input: {X[i]} â†’ Pred: {pred:.4f} (â‰ˆ{rounded}) | Expected: {expected} {status}\")\n",
    "\n",
    "accuracy = np.mean(np.round(final_preds) == y) * 100\n",
    "print(f\"\\nðŸŽ‰ Accuracy: {accuracy:.0f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "10b7c086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¾ Model saved to 'scratch_neural_network.pkl'\n",
      "ðŸ“‚ Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Save the neural network\n",
    "import pickle\n",
    "\n",
    "nn_model = {'W1': W1_t, 'b1': b1_t, 'W2': W2_t, 'b2': b2_t}\n",
    "\n",
    "with open('scratch_neural_network.pkl', 'wb') as f:\n",
    "    pickle.dump(nn_model, f)\n",
    "print(\"ðŸ’¾ Model saved to 'scratch_neural_network.pkl'\")\n",
    "\n",
    "# Load and verify\n",
    "with open('scratch_neural_network.pkl', 'rb') as f:\n",
    "    loaded_nn = pickle.load(f)\n",
    "print(\"ðŸ“‚ Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3985b0",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 5: PyTorch Regression Model ðŸ“ˆ\n",
    "\n",
    "**Regression** predicts a continuous number (like price, temperature, score)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "38bc5e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "5e30d85f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Created 'housing_data.csv'\n",
      "   square_feet  bedrooms  bathrooms  age          price\n",
      "0         3674         2          1   27  617629.746207\n",
      "1         1360         1          2   42  248646.597734\n",
      "2         1794         2          1   40  332280.626977\n",
      "3         1630         1          1   38  275989.794191\n",
      "4         1595         4          2   21  366530.439184\n"
     ]
    }
   ],
   "source": [
    "# Create Housing Data CSV\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "\n",
    "square_feet = np.random.randint(500, 4000, n_samples)\n",
    "bedrooms = np.random.randint(1, 6, n_samples)\n",
    "bathrooms = np.random.randint(1, 4, n_samples)\n",
    "age = np.random.randint(0, 50, n_samples)\n",
    "price = (square_feet * 150 + bedrooms * 15000 + bathrooms * 10000 +\n",
    "         (50 - age) * 1000 + np.random.randn(n_samples) * 20000)\n",
    "\n",
    "housing_df = pd.DataFrame({\n",
    "    'square_feet': square_feet, 'bedrooms': bedrooms,\n",
    "    'bathrooms': bathrooms, 'age': age, 'price': price\n",
    "})\n",
    "housing_df.to_csv('housing_data.csv', index=False)\n",
    "print(\"ðŸ“Š Created 'housing_data.csv'\")\n",
    "print(housing_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "275ca62a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 800 | Testing: 200\n"
     ]
    }
   ],
   "source": [
    "# Prepare data for PyTorch\n",
    "df = pd.read_csv('housing_data.csv')\n",
    "X_data = df[['square_feet', 'bedrooms', 'bathrooms', 'age']].values\n",
    "y_data = df['price'].values.reshape(-1, 1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "X_train_s = scaler_X.fit_transform(X_train)\n",
    "X_test_s = scaler_X.transform(X_test)\n",
    "y_train_s = scaler_y.fit_transform(y_train)\n",
    "y_test_s = scaler_y.transform(y_test)\n",
    "\n",
    "X_train_t = torch.FloatTensor(X_train_s)\n",
    "y_train_t = torch.FloatTensor(y_train_s)\n",
    "X_test_t = torch.FloatTensor(X_test_s)\n",
    "y_test_t = torch.FloatTensor(y_test_s)\n",
    "\n",
    "print(f\"Training: {len(X_train)} | Testing: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "7832a43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ  House Price Predictor:\n",
      "HousePricePredictor(\n",
      "  (layer1): Linear(in_features=4, out_features=64, bias=True)\n",
      "  (layer2): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (layer3): Linear(in_features=32, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class HousePricePredictor(nn.Module):\n",
    "    \"\"\"\n",
    "    A neural network model for predicting house prices.\n",
    "    \n",
    "    This class demonstrates how to create a PyTorch neural network for\n",
    "    regression tasks. It uses three fully connected layers with ReLU\n",
    "    activation functions to learn the relationship between house features\n",
    "    and prices.\n",
    "    \n",
    "    Architecture:\n",
    "        Input (4 features) â†’ Linear(64) â†’ ReLU â†’ Linear(32) â†’ ReLU â†’ Linear(1)\n",
    "    \n",
    "    Attributes:\n",
    "        layer1 (nn.Linear): First fully connected layer transforming 4 inputs to 64 neurons.\n",
    "        layer2 (nn.Linear): Second fully connected layer transforming 64 to 32 neurons.\n",
    "        layer3 (nn.Linear): Output layer transforming 32 neurons to 1 price prediction.\n",
    "        relu (nn.ReLU): ReLU activation function applied between layers.\n",
    "    \n",
    "    Example:\n",
    "        >>> model = HousePricePredictor(input_size=4)\n",
    "        >>> features = torch.tensor([[2000, 3, 2, 10]], dtype=torch.float32)\n",
    "        >>> predicted_price = model(features)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size):\n",
    "        \"\"\"\n",
    "        Initialize the HousePricePredictor model.\n",
    "        \n",
    "        Creates the neural network architecture with three linear layers\n",
    "        and ReLU activation. The network progressively reduces dimensionality\n",
    "        from input_size â†’ 64 â†’ 32 â†’ 1.\n",
    "        \n",
    "        Parameters:\n",
    "            input_size (int): Number of input features. For housing data,\n",
    "                this is typically 4 (square_feet, bedrooms, bathrooms, age).\n",
    "        \"\"\"\n",
    "        super(HousePricePredictor, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, 64)\n",
    "        self.layer2 = nn.Linear(64, 32)\n",
    "        self.layer3 = nn.Linear(32, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Perform forward pass through the network.\n",
    "        \n",
    "        Takes input features and passes them through the network layers\n",
    "        to produce a price prediction. ReLU activation is applied after\n",
    "        the first two layers but not the output layer.\n",
    "        \n",
    "        Parameters:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, input_size)\n",
    "                containing the house features.\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Predicted house prices of shape (batch_size, 1).\n",
    "                Note: Output is scaled if input was scaled.\n",
    "        \"\"\"\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.relu(self.layer2(x))\n",
    "        return self.layer3(x)\n",
    "\n",
    "model = HousePricePredictor(input_size=4)\n",
    "print(\"ðŸ  House Price Predictor:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "a5da8c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Training...\n",
      "Epoch 100 | Loss: 0.014738\n",
      "Epoch 200 | Loss: 0.013461\n",
      "Epoch 300 | Loss: 0.012802\n",
      "Epoch 400 | Loss: 0.012176\n",
      "Epoch 500 | Loss: 0.011676\n",
      "âœ… Training Complete!\n"
     ]
    }
   ],
   "source": [
    "# Train the regression model\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "print(\"ðŸš€ Training...\")\n",
    "for epoch in range(500):\n",
    "    model.train()\n",
    "    preds = model(X_train_t)\n",
    "    loss = criterion(preds, y_train_t)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch {epoch+1:3d} | Loss: {loss.item():.6f}\")\n",
    "\n",
    "print(\"âœ… Training Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "1618227a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: $18,075\n",
      "\n",
      "ðŸ  Sample Predictions:\n",
      "Predicted: $   596,514 | Actual: $   625,896\n",
      "Predicted: $   637,994 | Actual: $   641,823\n",
      "Predicted: $   622,952 | Actual: $   579,123\n",
      "Predicted: $   626,620 | Actual: $   656,709\n",
      "Predicted: $   394,016 | Actual: $   347,727\n"
     ]
    }
   ],
   "source": [
    "# Evaluate regression model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_preds = model(X_test_t)\n",
    "\n",
    "pred_prices = scaler_y.inverse_transform(test_preds.numpy())\n",
    "actual_prices = scaler_y.inverse_transform(y_test_s)\n",
    "\n",
    "mae = np.mean(np.abs(pred_prices - actual_prices))\n",
    "print(f\"Mean Absolute Error: ${mae:,.0f}\")\n",
    "\n",
    "print(\"\\nðŸ  Sample Predictions:\")\n",
    "for i in range(5):\n",
    "    print(f\"Predicted: ${pred_prices[i][0]:>10,.0f} | Actual: ${actual_prices[i][0]:>10,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "52768c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¾ Model saved to 'house_price_model.pth'\n",
      "\n",
      "ðŸ  New Prediction: 2000 sqft, 3 bed, 2 bath, 10 yrs = $415,521\n"
     ]
    }
   ],
   "source": [
    "# Save regression model\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'scaler_X': scaler_X,\n",
    "    'scaler_y': scaler_y,\n",
    "}, 'house_price_model.pth')\n",
    "print(\"ðŸ’¾ Model saved to 'house_price_model.pth'\")\n",
    "\n",
    "# Load and test\n",
    "loaded = torch.load('house_price_model.pth', weights_only=False)\n",
    "loaded_model = HousePricePredictor(input_size=4)\n",
    "loaded_model.load_state_dict(loaded['model_state_dict'])\n",
    "loaded_model.eval()\n",
    "\n",
    "# Predict new house\n",
    "new_house = np.array([[2000, 3, 2, 10]])\n",
    "new_scaled = loaded['scaler_X'].transform(new_house)\n",
    "with torch.no_grad():\n",
    "    pred = loaded_model(torch.FloatTensor(new_scaled))\n",
    "    price = loaded['scaler_y'].inverse_transform(pred.numpy())\n",
    "\n",
    "print(f\"\\nðŸ  New Prediction: 2000 sqft, 3 bed, 2 bath, 10 yrs = ${price[0][0]:,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162d7b2a",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 6: PyTorch Classification Model ðŸ·ï¸\n",
    "\n",
    "**Classification** predicts a category (like pass/fail, cat/dog, spam/not spam)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "3783757d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Created 'student_data.csv'\n",
      "Passed: 500 | Failed: 500\n"
     ]
    }
   ],
   "source": [
    "# Create Student Data CSV\n",
    "np.random.seed(42)\n",
    "n = 1000\n",
    "\n",
    "study_hours = np.random.uniform(0, 10, n)\n",
    "attendance = np.random.uniform(50, 100, n)\n",
    "prev_grade = np.random.uniform(40, 100, n)\n",
    "sleep_hours = np.random.uniform(4, 10, n)\n",
    "\n",
    "score = (study_hours * 8 + attendance * 0.5 + prev_grade * 0.3 +\n",
    "         sleep_hours * 3 + np.random.randn(n) * 10)\n",
    "passed = (score > np.median(score)).astype(int)\n",
    "\n",
    "student_df = pd.DataFrame({\n",
    "    'study_hours': study_hours, 'attendance': attendance,\n",
    "    'prev_grade': prev_grade, 'sleep_hours': sleep_hours, 'passed': passed\n",
    "})\n",
    "student_df.to_csv('student_data.csv', index=False)\n",
    "print(\"ðŸ“Š Created 'student_data.csv'\")\n",
    "print(f\"Passed: {passed.sum()} | Failed: {n - passed.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "af14816c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 800 | Testing: 200\n"
     ]
    }
   ],
   "source": [
    "# Prepare classification data\n",
    "df_c = pd.read_csv('student_data.csv')\n",
    "X_c = df_c[['study_hours', 'attendance', 'prev_grade', 'sleep_hours']].values\n",
    "y_c = df_c['passed'].values\n",
    "\n",
    "X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(X_c, y_c, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler_c = StandardScaler()\n",
    "X_train_cs = scaler_c.fit_transform(X_train_c)\n",
    "X_test_cs = scaler_c.transform(X_test_c)\n",
    "\n",
    "X_train_ct = torch.FloatTensor(X_train_cs)\n",
    "y_train_ct = torch.FloatTensor(y_train_c).unsqueeze(1)\n",
    "X_test_ct = torch.FloatTensor(X_test_cs)\n",
    "y_test_ct = torch.FloatTensor(y_test_c).unsqueeze(1)\n",
    "\n",
    "print(f\"Training: {len(X_train_c)} | Testing: {len(X_test_c)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "a22341ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ“ Student Classifier:\n",
      "StudentClassifier(\n",
      "  (layer1): Linear(in_features=4, out_features=32, bias=True)\n",
      "  (layer2): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (layer3): Linear(in_features=16, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class StudentClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    A neural network binary classifier for predicting student pass/fail.\n",
    "    \n",
    "    This class demonstrates a classification model in PyTorch. Unlike regression,\n",
    "    the output uses a sigmoid activation to produce a probability between 0 and 1,\n",
    "    which represents the likelihood of passing.\n",
    "    \n",
    "    Architecture:\n",
    "        Input (4 features) â†’ Linear(32) â†’ ReLU â†’ Linear(16) â†’ ReLU â†’ Linear(1) â†’ Sigmoid\n",
    "    \n",
    "    Attributes:\n",
    "        layer1 (nn.Linear): First fully connected layer, 4 inputs to 32 neurons.\n",
    "        layer2 (nn.Linear): Second fully connected layer, 32 to 16 neurons.\n",
    "        layer3 (nn.Linear): Output layer, 16 neurons to 1 output.\n",
    "        relu (nn.ReLU): ReLU activation for hidden layers.\n",
    "        sigmoid (nn.Sigmoid): Sigmoid activation for output probability.\n",
    "    \n",
    "    Example:\n",
    "        >>> classifier = StudentClassifier(input_size=4)\n",
    "        >>> features = torch.tensor([[6, 85, 75, 7]], dtype=torch.float32)\n",
    "        >>> pass_probability = classifier(features)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size):\n",
    "        \"\"\"\n",
    "        Initialize the StudentClassifier model.\n",
    "        \n",
    "        Creates the neural network with three linear layers, ReLU activations\n",
    "        for hidden layers, and sigmoid activation for the output to produce\n",
    "        a probability between 0 and 1.\n",
    "        \n",
    "        Parameters:\n",
    "            input_size (int): Number of input features. For student data,\n",
    "                this is typically 4 (study_hours, attendance, prev_grade, sleep_hours).\n",
    "        \"\"\"\n",
    "        super(StudentClassifier, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, 32)\n",
    "        self.layer2 = nn.Linear(32, 16)\n",
    "        self.layer3 = nn.Linear(16, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Perform forward pass through the classifier network.\n",
    "        \n",
    "        Passes input through hidden layers with ReLU activation, then\n",
    "        applies sigmoid to the output to produce a probability.\n",
    "        \n",
    "        Parameters:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, input_size)\n",
    "                containing the student features.\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Probability of passing, shape (batch_size, 1),\n",
    "                with values between 0 and 1. Values > 0.5 typically\n",
    "                indicate a \"pass\" prediction.\n",
    "        \"\"\"\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.relu(self.layer2(x))\n",
    "        return self.sigmoid(self.layer3(x))\n",
    "\n",
    "classifier = StudentClassifier(input_size=4)\n",
    "print(\"ðŸŽ“ Student Classifier:\")\n",
    "print(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "45fd4565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Training...\n",
      "Epoch  60 | Loss: 0.2003 | Acc: 90.6%\n",
      "Epoch 120 | Loss: 0.1762 | Acc: 91.6%\n",
      "Epoch 180 | Loss: 0.1529 | Acc: 92.6%\n",
      "Epoch 240 | Loss: 0.1310 | Acc: 93.5%\n",
      "Epoch 300 | Loss: 0.1168 | Acc: 94.6%\n",
      "âœ… Training Complete!\n"
     ]
    }
   ],
   "source": [
    "# Train classifier\n",
    "criterion_c = nn.BCELoss()\n",
    "optimizer_c = optim.Adam(classifier.parameters(), lr=0.01)\n",
    "\n",
    "print(\"ðŸš€ Training...\")\n",
    "for epoch in range(300):\n",
    "    classifier.train()\n",
    "    preds = classifier(X_train_ct)\n",
    "    loss = criterion_c(preds, y_train_ct)\n",
    "    \n",
    "    optimizer_c.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer_c.step()\n",
    "    \n",
    "    if (epoch + 1) % 60 == 0:\n",
    "        acc = ((preds > 0.5).float() == y_train_ct).float().mean()\n",
    "        print(f\"Epoch {epoch+1:3d} | Loss: {loss.item():.4f} | Acc: {acc.item()*100:.1f}%\")\n",
    "\n",
    "print(\"âœ… Training Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "f211b30f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Test Accuracy: 86.5%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate classifier\n",
    "classifier.eval()\n",
    "with torch.no_grad():\n",
    "    test_preds = classifier(X_test_ct)\n",
    "    test_labels = (test_preds > 0.5).float()\n",
    "    acc = (test_labels == y_test_ct).float().mean()\n",
    "\n",
    "print(f\"ðŸ“Š Test Accuracy: {acc.item()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "797ce760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¾ Model saved to 'student_classifier.pth'\n",
      "\n",
      "ðŸŽ“ Student (6hrs study, 85% attend, 75 prev, 7hrs sleep)\n",
      "Pass Probability: 94.5%\n",
      "Prediction: PASS âœ…\n"
     ]
    }
   ],
   "source": [
    "# Save classifier\n",
    "torch.save({\n",
    "    'model_state_dict': classifier.state_dict(),\n",
    "    'scaler': scaler_c\n",
    "}, 'student_classifier.pth')\n",
    "print(\"ðŸ’¾ Model saved to 'student_classifier.pth'\")\n",
    "\n",
    "# Load and predict\n",
    "loaded_c = torch.load('student_classifier.pth', weights_only=False)\n",
    "loaded_clf = StudentClassifier(input_size=4)\n",
    "loaded_clf.load_state_dict(loaded_c['model_state_dict'])\n",
    "loaded_clf.eval()\n",
    "\n",
    "new_student = np.array([[6, 85, 75, 7]])\n",
    "new_s = loaded_c['scaler'].transform(new_student)\n",
    "with torch.no_grad():\n",
    "    prob = loaded_clf(torch.FloatTensor(new_s))\n",
    "\n",
    "print(f\"\\nðŸŽ“ Student (6hrs study, 85% attend, 75 prev, 7hrs sleep)\")\n",
    "print(f\"Pass Probability: {prob.item()*100:.1f}%\")\n",
    "print(f\"Prediction: {'PASS âœ…' if prob.item() > 0.5 else 'FAIL âŒ'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31edb4ad",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 7: GPT from Scratch! ðŸ¤–\n",
    "\n",
    "Building a mini GPT (Generative Pre-trained Transformer)!\n",
    "\n",
    "GPT uses:\n",
    "- **Tokenization** - Converting text to numbers\n",
    "- **Embeddings** - Giving meaning to tokens\n",
    "- **Self-Attention** - Understanding context\n",
    "- **Transformer** - The architecture powering modern AI!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "fe80bf56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training text: 508 characters\n"
     ]
    }
   ],
   "source": [
    "# Training text for GPT\n",
    "text = \"\"\"\n",
    "The quick brown fox jumps over the lazy dog.\n",
    "Python is a great programming language for beginners.\n",
    "Machine learning helps computers learn from data.\n",
    "Neural networks are inspired by the human brain.\n",
    "Deep learning uses many layers of neurons.\n",
    "Artificial intelligence is transforming our world.\n",
    "Data science combines statistics and programming.\n",
    "The future of technology is exciting and full of possibilities.\n",
    "Learning to code opens many doors for young people.\n",
    "Practice makes perfect when learning new skills.\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Training text: {len(text)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "280627ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 37\n",
      "Example: 'learn' â†’ [22, 15, 11, 28, 24] â†’ 'learn'\n"
     ]
    }
   ],
   "source": [
    "# Tokenization with UNK token for unknown characters\n",
    "chars = sorted(list(set(text)))\n",
    "chars = ['<UNK>'] + chars  # Add unknown token at index 0\n",
    "vocab_size = len(chars)\n",
    "\n",
    "char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "\n",
    "def encode(s):\n",
    "    \"\"\"\n",
    "    Convert a string to a list of integer token indices.\n",
    "    \n",
    "    This function tokenizes text at the character level, mapping each\n",
    "    character to its corresponding integer index. Unknown characters\n",
    "    (not in the vocabulary) are mapped to index 0 (the <UNK> token).\n",
    "    \n",
    "    Parameters:\n",
    "        s (str): The input string to encode/tokenize.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of integer indices representing each character.\n",
    "            Each index corresponds to a position in the vocabulary.\n",
    "    \n",
    "    Example:\n",
    "        >>> encode(\"hi\")\n",
    "        [23, 24]  # Indices depend on vocabulary\n",
    "    \"\"\"\n",
    "    return [char_to_idx.get(c, 0) for c in s]\n",
    "\n",
    "\n",
    "def decode(indices):\n",
    "    \"\"\"\n",
    "    Convert a list of integer token indices back to a string.\n",
    "    \n",
    "    This function reverses the tokenization process, mapping each\n",
    "    integer index back to its corresponding character.\n",
    "    \n",
    "    Parameters:\n",
    "        indices (list): A list of integer indices to decode.\n",
    "    \n",
    "    Returns:\n",
    "        str: The decoded string reconstructed from the indices.\n",
    "    \n",
    "    Example:\n",
    "        >>> decode([23, 24])\n",
    "        \"hi\"\n",
    "    \"\"\"\n",
    "    return ''.join([idx_to_char[i] for i in indices])\n",
    "\n",
    "\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Example: 'learn' â†’ {encode('learn')} â†’ '{decode(encode('learn'))}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "2a254db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shapes: x=torch.Size([16, 32]), y=torch.Size([16, 32])\n",
      "Learning rate: 0.001\n"
     ]
    }
   ],
   "source": [
    "# GPT Hyperparameters\n",
    "block_size = 32       # Context window size\n",
    "batch_size = 16       # Number of sequences per batch\n",
    "embed_size = 64       # Embedding dimension\n",
    "n_heads = 4           # Number of attention heads\n",
    "n_layers = 2          # Number of transformer blocks\n",
    "learning_rate = 1e-3  # Learning rate for optimizer\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "\n",
    "def get_batch():\n",
    "    \"\"\"\n",
    "    Generate a random batch of training sequences for the GPT model.\n",
    "    \n",
    "    Creates input-target pairs where the target is the input shifted\n",
    "    by one position. This teaches the model to predict the next character.\n",
    "    \n",
    "    Parameters:\n",
    "        None (uses global data, block_size, batch_size variables)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - x (torch.Tensor): Input sequences of shape (batch_size, block_size)\n",
    "            - y (torch.Tensor): Target sequences of shape (batch_size, block_size)\n",
    "              where y[i] = x[i] shifted right by 1 position\n",
    "    \n",
    "    Example:\n",
    "        If x = \"The quick\", then y = \"he quick \" (next char for each position)\n",
    "    \"\"\"\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "\n",
    "xb, yb = get_batch()\n",
    "print(f\"Batch shapes: x={xb.shape}, y={yb.shape}\")\n",
    "print(f\"Learning rate: {learning_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "06222dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Head class defined!\n"
     ]
    }
   ],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\"\n",
    "    A single head of self-attention for the transformer.\n",
    "    \n",
    "    Self-attention allows each position in a sequence to attend to all\n",
    "    previous positions, learning which parts of the context are relevant\n",
    "    for predicting the next token. This implements scaled dot-product attention.\n",
    "    \n",
    "    Attributes:\n",
    "        key (nn.Linear): Linear projection for computing keys.\n",
    "        query (nn.Linear): Linear projection for computing queries.\n",
    "        value (nn.Linear): Linear projection for computing values.\n",
    "        tril (torch.Tensor): Lower triangular mask for causal attention.\n",
    "    \n",
    "    Example:\n",
    "        >>> head = Head(head_size=16)\n",
    "        >>> x = torch.randn(2, 32, 64)  # (batch, seq_len, embed_size)\n",
    "        >>> output = head(x)  # (batch, seq_len, head_size)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, head_size):\n",
    "        \"\"\"\n",
    "        Initialize a single attention head.\n",
    "        \n",
    "        Creates the key, query, and value projection layers and registers\n",
    "        the causal mask as a buffer (not a parameter).\n",
    "        \n",
    "        Parameters:\n",
    "            head_size (int): The dimension of the key/query/value projections.\n",
    "                Typically embed_size // n_heads.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(embed_size, head_size, bias=False)\n",
    "        self.query = nn.Linear(embed_size, head_size, bias=False)\n",
    "        self.value = nn.Linear(embed_size, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Apply scaled dot-product self-attention to the input.\n",
    "        \n",
    "        Computes attention weights between all positions, applies causal\n",
    "        masking to prevent attending to future positions, and returns\n",
    "        the weighted sum of values.\n",
    "        \n",
    "        Parameters:\n",
    "            x (torch.Tensor): Input tensor of shape (batch, seq_len, embed_size).\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (batch, seq_len, head_size)\n",
    "                after applying self-attention.\n",
    "        \n",
    "        Process:\n",
    "            1. Project input to keys, queries, values\n",
    "            2. Compute attention scores: Q @ K^T / sqrt(d_k)\n",
    "            3. Apply causal mask (prevent attending to future)\n",
    "            4. Softmax to get attention weights\n",
    "            5. Weighted sum of values\n",
    "        \"\"\"\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        \n",
    "        weights = q @ k.transpose(-2, -1) * (C ** -0.5)\n",
    "        weights = weights.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        weights = torch.softmax(weights, dim=-1)\n",
    "        \n",
    "        v = self.value(x)\n",
    "        return weights @ v\n",
    "\n",
    "print(\"âœ… Head class defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "e1df2817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… MultiHeadAttention class defined!\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head self-attention mechanism for the transformer.\n",
    "    \n",
    "    Runs multiple attention heads in parallel and concatenates their outputs.\n",
    "    This allows the model to jointly attend to information from different\n",
    "    representation subspaces at different positions.\n",
    "    \n",
    "    Attributes:\n",
    "        heads (nn.ModuleList): List of Head modules running in parallel.\n",
    "        proj (nn.Linear): Output projection to combine head outputs.\n",
    "    \n",
    "    Example:\n",
    "        >>> mha = MultiHeadAttention(num_heads=4, head_size=16)\n",
    "        >>> x = torch.randn(2, 32, 64)\n",
    "        >>> output = mha(x)  # (2, 32, 64)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_heads, head_size):\n",
    "        \"\"\"\n",
    "        Initialize multi-head attention with the specified number of heads.\n",
    "        \n",
    "        Creates multiple attention heads and an output projection layer\n",
    "        to combine their outputs back to the embedding dimension.\n",
    "        \n",
    "        Parameters:\n",
    "            num_heads (int): Number of parallel attention heads.\n",
    "            head_size (int): Dimension of each attention head.\n",
    "                num_heads * head_size should equal embed_size.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(embed_size, embed_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Apply multi-head attention to the input.\n",
    "        \n",
    "        Runs all attention heads in parallel, concatenates their outputs,\n",
    "        and projects back to the embedding dimension.\n",
    "        \n",
    "        Parameters:\n",
    "            x (torch.Tensor): Input tensor of shape (batch, seq_len, embed_size).\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (batch, seq_len, embed_size)\n",
    "                after multi-head attention and output projection.\n",
    "        \"\"\"\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        return self.proj(out)\n",
    "\n",
    "print(\"âœ… MultiHeadAttention class defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "97ab1ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… FeedForward class defined!\n"
     ]
    }
   ],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Position-wise feed-forward network for the transformer.\n",
    "    \n",
    "    A simple two-layer neural network applied independently to each position.\n",
    "    This is the \"feed-forward\" part of the transformer block that processes\n",
    "    each position's representation after the attention step.\n",
    "    \n",
    "    Architecture:\n",
    "        Linear(embed_size â†’ 4*embed_size) â†’ ReLU â†’ Linear(4*embed_size â†’ embed_size)\n",
    "    \n",
    "    Attributes:\n",
    "        net (nn.Sequential): The feed-forward network layers.\n",
    "    \n",
    "    Example:\n",
    "        >>> ffn = FeedForward(embed_size=64)\n",
    "        >>> x = torch.randn(2, 32, 64)\n",
    "        >>> output = ffn(x)  # (2, 32, 64)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embed_size):\n",
    "        \"\"\"\n",
    "        Initialize the feed-forward network.\n",
    "        \n",
    "        Creates a two-layer MLP with an expansion factor of 4 in the hidden\n",
    "        layer, which is standard in transformer architectures.\n",
    "        \n",
    "        Parameters:\n",
    "            embed_size (int): The embedding dimension (input and output size).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embed_size, 4 * embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * embed_size, embed_size),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Apply the feed-forward network to the input.\n",
    "        \n",
    "        Processes each position independently through the two-layer MLP.\n",
    "        \n",
    "        Parameters:\n",
    "            x (torch.Tensor): Input tensor of shape (batch, seq_len, embed_size).\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of same shape after feed-forward processing.\n",
    "        \"\"\"\n",
    "        return self.net(x)\n",
    "\n",
    "print(\"âœ… FeedForward class defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "54f214a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… TransformerBlock class defined!\n"
     ]
    }
   ],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A single transformer block with self-attention and feed-forward layers.\n",
    "    \n",
    "    This block implements the standard transformer architecture with pre-norm\n",
    "    layer normalization, multi-head self-attention with residual connection,\n",
    "    and a feed-forward network with residual connection.\n",
    "    \n",
    "    Architecture:\n",
    "        x â†’ LayerNorm â†’ MultiHeadAttention â†’ + x â†’ LayerNorm â†’ FeedForward â†’ + x\n",
    "    \n",
    "    Attributes:\n",
    "        attention (MultiHeadAttention): The multi-head attention layer.\n",
    "        ffn (FeedForward): The position-wise feed-forward network.\n",
    "        ln1 (nn.LayerNorm): Layer normalization before attention.\n",
    "        ln2 (nn.LayerNorm): Layer normalization before feed-forward.\n",
    "    \n",
    "    Example:\n",
    "        >>> block = TransformerBlock(embed_size=64, n_heads=4)\n",
    "        >>> x = torch.randn(2, 32, 64)\n",
    "        >>> output = block(x)  # (2, 32, 64)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embed_size, n_heads):\n",
    "        \"\"\"\n",
    "        Initialize the transformer block.\n",
    "        \n",
    "        Creates the multi-head attention, feed-forward network, and\n",
    "        layer normalization components.\n",
    "        \n",
    "        Parameters:\n",
    "            embed_size (int): The embedding dimension.\n",
    "            n_heads (int): Number of attention heads. embed_size must be\n",
    "                divisible by n_heads.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        head_size = embed_size // n_heads\n",
    "        self.attention = MultiHeadAttention(n_heads, head_size)\n",
    "        self.ffn = FeedForward(embed_size)\n",
    "        self.ln1 = nn.LayerNorm(embed_size)\n",
    "        self.ln2 = nn.LayerNorm(embed_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Apply the transformer block to the input.\n",
    "        \n",
    "        Applies layer norm, attention with residual, then layer norm\n",
    "        and feed-forward with residual.\n",
    "        \n",
    "        Parameters:\n",
    "            x (torch.Tensor): Input tensor of shape (batch, seq_len, embed_size).\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of same shape after transformer processing.\n",
    "        \"\"\"\n",
    "        x = x + self.attention(self.ln1(x))\n",
    "        x = x + self.ffn(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "print(\"âœ… TransformerBlock class defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "74d54557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¤– Mini GPT Created! Parameters: 106,533\n"
     ]
    }
   ],
   "source": [
    "class MiniGPT(nn.Module):\n",
    "    \"\"\"\n",
    "    A small GPT model for character-level text generation.\n",
    "    \n",
    "    This implements a decoder-only transformer that learns to predict\n",
    "    the next character given a sequence of previous characters. It uses\n",
    "    token embeddings, positional embeddings, transformer blocks, and\n",
    "    a final linear layer to produce logits over the vocabulary.\n",
    "    \n",
    "    Architecture:\n",
    "        Token Embedding + Position Embedding â†’ N Ã— TransformerBlock â†’ LayerNorm â†’ Linear\n",
    "    \n",
    "    Attributes:\n",
    "        token_embedding (nn.Embedding): Embedding layer for input tokens.\n",
    "        position_embedding (nn.Embedding): Embedding layer for position indices.\n",
    "        blocks (nn.Sequential): Stack of transformer blocks.\n",
    "        ln_final (nn.LayerNorm): Final layer normalization.\n",
    "        output (nn.Linear): Output projection to vocabulary logits.\n",
    "    \n",
    "    Example:\n",
    "        >>> gpt = MiniGPT()\n",
    "        >>> x = torch.tensor([[1, 2, 3, 4]])\n",
    "        >>> logits, loss = gpt(x, targets=torch.tensor([[2, 3, 4, 5]]))\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the MiniGPT model.\n",
    "        \n",
    "        Creates token embeddings, positional embeddings, transformer blocks,\n",
    "        final layer norm, and output projection. Uses global hyperparameters\n",
    "        vocab_size, embed_size, block_size, n_heads, and n_layers.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.position_embedding = nn.Embedding(block_size, embed_size)\n",
    "        self.blocks = nn.Sequential(*[\n",
    "            TransformerBlock(embed_size, n_heads) for _ in range(n_layers)\n",
    "        ])\n",
    "        self.ln_final = nn.LayerNorm(embed_size)\n",
    "        self.output = nn.Linear(embed_size, vocab_size)\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        \"\"\"\n",
    "        Perform forward pass through the GPT model.\n",
    "        \n",
    "        Computes token and position embeddings, passes through transformer\n",
    "        blocks, and produces logits. Optionally computes cross-entropy loss.\n",
    "        \n",
    "        Parameters:\n",
    "            idx (torch.Tensor): Input token indices of shape (batch, seq_len).\n",
    "            targets (torch.Tensor, optional): Target token indices for computing\n",
    "                loss, same shape as idx but shifted by one position.\n",
    "        \n",
    "        Returns:\n",
    "            tuple: A tuple containing:\n",
    "                - logits (torch.Tensor): Output logits of shape (batch, seq_len, vocab_size)\n",
    "                - loss (torch.Tensor or None): Cross-entropy loss if targets provided\n",
    "        \"\"\"\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding(idx)\n",
    "        pos_emb = self.position_embedding(torch.arange(T))\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_final(x)\n",
    "        logits = self.output(x)\n",
    "        \n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            B, T, C = logits.shape\n",
    "            logits_flat = logits.view(B*T, C)\n",
    "            targets_flat = targets.view(B*T)\n",
    "            loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        \"\"\"\n",
    "        Generate new tokens autoregressively.\n",
    "        \n",
    "        Starting from the given context, generates new tokens one at a time\n",
    "        by sampling from the model's output distribution.\n",
    "        \n",
    "        Parameters:\n",
    "            idx (torch.Tensor): Starting context of shape (batch, seq_len).\n",
    "            max_new_tokens (int): Number of new tokens to generate.\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Extended sequence of shape (batch, seq_len + max_new_tokens).\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "\n",
    "gpt = MiniGPT()\n",
    "print(f\"ðŸ¤– Mini GPT Created! Parameters: {sum(p.numel() for p in gpt.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f431caa3",
   "metadata": {},
   "source": [
    "### Training the GPT\n",
    "\n",
    "Now let's train our GPT to learn the patterns in the text!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "025ff9cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ‹ï¸ Training Mini GPT...\n",
      "----------------------------------------\n",
      "Step    0 | Loss: 3.8210\n",
      "Step  200 | Loss: 1.5639\n",
      "Step  400 | Loss: 0.4786\n",
      "Step  600 | Loss: 0.1955\n",
      "Step  800 | Loss: 0.1388\n",
      "Step 1000 | Loss: 0.1291\n",
      "Step 1200 | Loss: 0.1284\n",
      "Step 1400 | Loss: 0.1342\n",
      "Step 1600 | Loss: 0.1308\n",
      "Step 1800 | Loss: 0.1186\n",
      "----------------------------------------\n",
      "âœ… Training complete! Final loss: 0.1134\n"
     ]
    }
   ],
   "source": [
    "# Training loop for GPT\n",
    "optimizer = torch.optim.AdamW(gpt.parameters(), lr=learning_rate)\n",
    "\n",
    "print(\"ðŸ‹ï¸ Training Mini GPT...\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for step in range(2000):\n",
    "    # Get a batch of data\n",
    "    xb, yb = get_batch()\n",
    "    \n",
    "    # Forward pass and compute loss\n",
    "    logits, loss = gpt(xb, yb)\n",
    "    \n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Print progress every 200 steps\n",
    "    if step % 200 == 0:\n",
    "        print(f\"Step {step:4d} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "print(\"-\" * 40)\n",
    "print(f\"âœ… Training complete! Final loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13b3fae",
   "metadata": {},
   "source": [
    "### Generate Text with GPT\n",
    "\n",
    "Let's use our trained model to generate new text!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "e7cd95a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ Generating text from prompts:\n",
      "==================================================\n",
      "\n",
      "ðŸ”¹ Prompt: 'Machine'\n",
      "----------------------------------------\n",
      "Machine learning helps computers learn from data.\n",
      "Neural networks are inspired by the h\n",
      "\n",
      "\n",
      "ðŸ”¹ Prompt: 'Python'\n",
      "----------------------------------------\n",
      "Python is a great programming langre furebe of technology is exciting and full of poss\n",
      "\n",
      "\n",
      "ðŸ”¹ Prompt: 'Data'\n",
      "----------------------------------------\n",
      "Data science combines statistics and programming.\n",
      "The future of technology is exciti\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_text(prompt, max_tokens=100):\n",
    "    \"\"\"\n",
    "    Generate text from a prompt using the trained GPT model.\n",
    "    \n",
    "    Takes a text prompt, encodes it, feeds it to the model,\n",
    "    and decodes the generated tokens back to text.\n",
    "    \n",
    "    Parameters:\n",
    "        prompt (str): The starting text to generate from.\n",
    "        max_tokens (int): Maximum number of new tokens to generate.\n",
    "            Default is 100.\n",
    "    \n",
    "    Returns:\n",
    "        str: The generated text including the original prompt.\n",
    "    \n",
    "    Example:\n",
    "        >>> generate_text(\"Machine learning is\", max_tokens=50)\n",
    "        'Machine learning is the study of algorithms...'\n",
    "    \"\"\"\n",
    "    # Encode the prompt\n",
    "    context = torch.tensor([encode(prompt)])\n",
    "    \n",
    "    # Generate tokens\n",
    "    generated = gpt.generate(context, max_new_tokens=max_tokens)\n",
    "    \n",
    "    # Decode and return\n",
    "    return decode(generated[0].tolist())\n",
    "\n",
    "# Test generation with different prompts\n",
    "print(\"ðŸ“ Generating text from prompts:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "prompts = [\"Machine\", \"Python\", \"Data\"]\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"\\nðŸ”¹ Prompt: '{prompt}'\")\n",
    "    print(\"-\" * 40)\n",
    "    generated = generate_text(prompt, max_tokens=80)\n",
    "    print(generated)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109c472b",
   "metadata": {},
   "source": [
    "### Save and Load GPT Model\n",
    "\n",
    "Let's save our trained GPT and demonstrate loading it back!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "5619c2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¾ GPT model saved to 'mini_gpt_model.pth'!\n",
      "âœ… GPT model loaded successfully!\n",
      "\n",
      "ðŸ“ Testing loaded model:\n",
      "Prompt: 'Machine'\n",
      "Generated: Machine learning helps computers learn \n"
     ]
    }
   ],
   "source": [
    "# Save the GPT model\n",
    "torch.save({\n",
    "    'model_state_dict': gpt.state_dict(),\n",
    "    'vocab_size': vocab_size,\n",
    "    'embed_size': embed_size,\n",
    "    'block_size': block_size,\n",
    "    'n_heads': n_heads,\n",
    "    'n_layers': n_layers,\n",
    "    'char_to_idx': char_to_idx,\n",
    "    'idx_to_char': idx_to_char\n",
    "}, 'mini_gpt_model.pth')\n",
    "\n",
    "print(\"ðŸ’¾ GPT model saved to 'mini_gpt_model.pth'!\")\n",
    "\n",
    "# Load the model to verify\n",
    "checkpoint = torch.load('mini_gpt_model.pth')\n",
    "\n",
    "loaded_gpt = MiniGPT()\n",
    "loaded_gpt.load_state_dict(checkpoint['model_state_dict'])\n",
    "loaded_gpt.eval()\n",
    "\n",
    "print(\"âœ… GPT model loaded successfully!\")\n",
    "\n",
    "# Test the loaded model\n",
    "print(\"\\nðŸ“ Testing loaded model:\")\n",
    "test_prompt = \"Machine\"\n",
    "context = torch.tensor([encode(test_prompt)])\n",
    "generated = loaded_gpt.generate(context, max_new_tokens=32)\n",
    "print(f\"Prompt: '{test_prompt}'\")\n",
    "print(f\"Generated: {decode(generated[0].tolist())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a9de4c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ðŸŽ‰ Congratulations! You've Completed the Full Mini-Course!\n",
    "\n",
    "## What You Learned:\n",
    "\n",
    "### Part 1: Python Basics\n",
    "- Input/Output with `print()` and `input()`\n",
    "- Variables and data types (int, float, str, bool)\n",
    "- Constants (naming convention)\n",
    "\n",
    "### Part 2: Data Structures\n",
    "- **Lists**: Ordered, mutable collections `[1, 2, 3]`\n",
    "- **Tuples**: Ordered, immutable collections `(1, 2, 3)`\n",
    "- **Dictionaries**: Key-value pairs `{\"key\": \"value\"}`\n",
    "\n",
    "### Part 3: Functions and Classes\n",
    "- Functions with parameters, returns, and docstrings\n",
    "- Classes with `__init__`, methods, and attributes\n",
    "- Object-oriented programming basics\n",
    "\n",
    "### Part 4: Neural Network from Scratch\n",
    "- Sigmoid activation function\n",
    "- Forward propagation\n",
    "- Mean Squared Error loss\n",
    "- Backpropagation and gradient descent\n",
    "- Training loop\n",
    "\n",
    "### Part 5: PyTorch Regression\n",
    "- Loading data with pandas\n",
    "- Feature scaling with StandardScaler\n",
    "- Building a regression model with nn.Module\n",
    "- Training and making predictions\n",
    "\n",
    "### Part 6: PyTorch Classification\n",
    "- Binary classification problem\n",
    "- Sigmoid output for probabilities\n",
    "- BCELoss for binary cross-entropy\n",
    "- Accuracy calculation\n",
    "\n",
    "### Part 7: GPT from Scratch\n",
    "- Character-level tokenization with UNK token\n",
    "- Self-attention mechanism\n",
    "- Multi-head attention\n",
    "- Transformer blocks\n",
    "- Text generation\n",
    "\n",
    "## Files Created:\n",
    "- `housing_data.csv` - Sample housing data\n",
    "- `student_data.csv` - Sample student data\n",
    "- `scratch_neural_network.pkl` - Neural network from scratch\n",
    "- `house_price_model.pth` - PyTorch regression model\n",
    "- `student_classifier.pth` - PyTorch classification model\n",
    "- `mini_gpt_model.pth` - Mini GPT model\n",
    "\n",
    "## Next Steps:\n",
    "1. ðŸ“š Read more about neural networks and deep learning\n",
    "2. ðŸ”¬ Experiment with different hyperparameters\n",
    "3. ðŸ“Š Try training on your own datasets\n",
    "4. ðŸš€ Explore more advanced architectures (CNNs, RNNs, Transformers)\n",
    "5. ðŸ¤— Check out Hugging Face for pre-trained models\n",
    "\n",
    "**Great job on completing this comprehensive guide!** ðŸŒŸ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
